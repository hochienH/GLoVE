activation: ReLU
dropout: 0.1
ff_size: 64
future_cov_dim: 2
hidden_size: 32
input_chunk_length: 90
input_dim: 2
likelihood: null
lr_scheduler_cls: !!python/name:torch.optim.lr_scheduler.ExponentialLR ''
lr_scheduler_kwargs:
  gamma: 0.99
norm_type: LayerNorm
normalize_before: false
nr_params: 1
num_blocks: 4
optimizer_cls: !!python/name:torch.optim.adam.Adam ''
optimizer_kwargs:
  lr: 0.0002
output_chunk_length: 1
output_chunk_shift: 0
output_dim: 2
past_cov_dim: 165
static_cov_dim: 46
train_sample_shape:
- !!python/tuple
  - 90
  - 2
- !!python/tuple
  - 90
  - 165
- !!python/tuple
  - 90
  - 2
- !!python/tuple
  - 1
  - 2
- !!python/tuple
  - 1
  - 46
- !!python/tuple
  - 1
  - 2
use_reversible_instance_norm: true
